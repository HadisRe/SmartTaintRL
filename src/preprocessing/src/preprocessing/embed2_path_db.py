import json
import os
from datetime import datetime
from collections import defaultdict, Counter


def create_nodes_dict(nodes_list):
    """
    ØªØ¨Ø¯ÛŒÙ„ Ù„ÛŒØ³Øª nodes Ø¨Ù‡ dictionary Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø³Ø±ÛŒØ¹
    """
    return {node['id']: node for node in nodes_list}


def extract_path_basic_info(path_data, debug=False):
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡ Ø§Ø² ÛŒÚ© path Ø¯Ø± taint analysis
    """
    info = {
        'path_nodes': path_data.get('path', []),
        'path_length': path_data.get('path_length', len(path_data.get('path', []))),
        'risk_level': path_data.get('risk_level', 'UNKNOWN'),  # Hidden - won't go to state
        'source_node': path_data.get('source', 'unknown'),
        'sink_node': path_data.get('sink', 'unknown'),
        'source_type': path_data.get('source_type', 'unknown'),
        'sink_type': path_data.get('sink_type', 'unknown'),
        'mitigation_count': len(path_data.get('mitigating_factors', [])),
        'risk_factors_count': len(path_data.get('risk_factors', [])),
        'has_data_flow': path_data.get('has_data_flow', False),
        'contains_loop': path_data.get('contains_loop', False),
        # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø­ØªÙˆØ§ Ø¨Ø±Ø§ÛŒ debugging ÙˆÙ„ÛŒ Ø¨Ù‡ agent Ù†Ù…ÛŒØ¯Ù‡ÛŒÙ…
        '_mitigation_content': path_data.get('mitigating_factors', []),
        '_risk_content': path_data.get('risk_factors', [])
    }

    if debug:
        print(f"    ğŸ“ Path: {info['source_node']} â†’ {info['sink_node']}")
        print(f"       Length: {info['path_length']}, Risk: {info['risk_level']}, "
              f"Mitigations: {info['mitigation_count']}")
        print(f"       Source Type: {info['source_type']}, Sink Type: {info['sink_type']}")

    return info


def enrich_from_semantic_graph(path_info, graph_data, debug=False):
    """
    ØºÙ†ÛŒâ€ŒØ³Ø§Ø²ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª path Ø§Ø² semantic graph
    """
    # ØªØ¨Ø¯ÛŒÙ„ Ù„ÛŒØ³Øª nodes Ø¨Ù‡ dictionary
    nodes_list = graph_data.get('nodes', [])
    nodes_dict = create_nodes_dict(nodes_list)

    enriched = {
        'nodes_detail': [],
        'node_types_count': defaultdict(int),
        'functions_involved': set(),
        'require_nodes_in_path': 0,
        'condition_nodes_in_path': 0,
        'keccak_nodes_in_path': 0,
        'assignment_nodes_in_path': 0
    }

    # Ø¨Ø±Ø§ÛŒ Ù‡Ø± node Ø¯Ø± path
    for node_id in path_info['path_nodes']:
        if node_id in nodes_dict:
            node_data = nodes_dict[node_id]

            # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù…Ù„ node
            node_detail = {
                'node_id': node_id,
                'type': node_data.get('type', 'unknown'),
                'function': node_data.get('function', 'unknown'),
                'is_source': node_data.get('is_source', False),
                'is_sink': node_data.get('is_sink', False),
                'code_snippet': node_data.get('code_snippet', '')[:100],  # ÙÙ‚Ø· 100 Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§ÙˆÙ„
                'label': node_data.get('label', ''),
                'used_vars': node_data.get('used_vars', []),
                'defined_vars': node_data.get('defined_vars', [])
            }
            enriched['nodes_detail'].append(node_detail)

            # Ø´Ù…Ø§Ø±Ø´ types
            node_type = node_data.get('type', 'unknown')
            enriched['node_types_count'][node_type] += 1

            # Ø´Ù…Ø§Ø±Ø´ specific types
            if 'require' in node_type.lower() or 'require' in node_data.get('label', '').lower():
                enriched['require_nodes_in_path'] += 1
            elif node_type == 'condition' or 'if' in node_type.lower():
                enriched['condition_nodes_in_path'] += 1
            elif 'keccak' in node_type.lower() or 'hash' in node_type.lower():
                enriched['keccak_nodes_in_path'] += 1
            elif node_type == 'assignment' or '=' in node_data.get('label', ''):
                enriched['assignment_nodes_in_path'] += 1

            # functions involved
            func_name = node_data.get('function', 'unknown')
            if func_name and func_name != 'unknown' and func_name != '':
                enriched['functions_involved'].add(func_name)
        else:
            # Ø§Ú¯Ø± node Ø¯Ø± graph Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯
            enriched['nodes_detail'].append({
                'node_id': node_id,
                'type': 'not_found',
                'function': 'unknown'
            })

    # ØªØ¨Ø¯ÛŒÙ„ set Ø¨Ù‡ list Ø¨Ø±Ø§ÛŒ JSON
    enriched['functions_involved'] = list(enriched['functions_involved'])
    enriched['unique_functions_count'] = len(enriched['functions_involved'])

    if debug:
        print(f"      ğŸ” Enriched: {len(enriched['nodes_detail'])} nodes detailed")
        print(f"         Functions: {enriched['unique_functions_count']} unique")
        print(f"         Node types: Require={enriched['require_nodes_in_path']}, "
              f"Condition={enriched['condition_nodes_in_path']}, "
              f"Keccak={enriched['keccak_nodes_in_path']}")

    return enriched


def extract_function_context(path_info, enriched_info, ast_data, graph_data, debug=False):
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ context Ø§Ø² function Ú©Ù‡ path Ø¯Ø± Ø¢Ù† Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯
    """
    context = {
        'primary_function': 'unknown',
        'function_has_modifier': False,
        'function_visibility': 'unknown',
        'function_require_count': 0,  # Ú©Ù„ requires Ø¯Ø± function
        'protection_outside_path': False,  # Ø¢ÛŒØ§ Ø®Ø§Ø±Ø¬ Ø§Ø² path Ù…Ø­Ø§ÙØ¸Øª Ù‡Ø³Øª
        'modifier_names': [],
        'function_state_changes': False
    }

    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† primary function (function Ú©Ù‡ Ø¨ÛŒØ´ØªØ± nodes Ø¯Ø± Ø¢Ù† Ù‡Ø³ØªÙ†Ø¯)
    if enriched_info['functions_involved']:
        # Ø´Ù…Ø§Ø±Ø´ ØªØ¹Ø¯Ø§Ø¯ nodes Ø¯Ø± Ù‡Ø± function
        func_counter = Counter()
        for node in enriched_info['nodes_detail']:
            func = node.get('function', 'unknown')
            if func and func != 'unknown':
                func_counter[func] += 1

        if func_counter:
            context['primary_function'] = func_counter.most_common(1)[0][0]

    # Ø§Ø·Ù„Ø§Ø¹Ø§Øª function Ø§Ø² AST
    if 'contracts' in ast_data and context['primary_function'] != 'unknown':
        for contract_name, contract_data in ast_data['contracts'].items():
            if 'functions' in contract_data:
                for func_name, func_data in contract_data['functions'].items():
                    if func_name == context['primary_function']:
                        # Modifiers
                        modifiers = func_data.get('modifiers', [])
                        context['function_has_modifier'] = len(modifiers) > 0
                        context['modifier_names'] = modifiers

                        # Visibility
                        context['function_visibility'] = func_data.get('visibility', 'unknown')

                        # State mutability
                        state_mut = func_data.get('state_mutability', '')
                        context['function_state_changes'] = state_mut not in ['view', 'pure']

                        # Ú©Ù„ requires Ø¯Ø± function body
                        body_str = json.dumps(func_data.get('body', {})).lower()
                        context['function_require_count'] = body_str.count('require')

                        # Ø¢ÛŒØ§ requires Ø¨ÛŒØ´ØªØ± Ø§Ø² Ø¢Ù†Ú†Ù‡ Ø¯Ø± path Ù‡Ø³Øª ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŸ
                        if context['function_require_count'] > enriched_info['require_nodes_in_path']:
                            context['protection_outside_path'] = True

                        break

    # Ø¨Ø±Ø±Ø³ÛŒ nodes Ø¯Ø± graph Ø¨Ø±Ø§ÛŒ protections Ø®Ø§Ø±Ø¬ Ø§Ø² path
    if not context['protection_outside_path'] and context['primary_function'] != 'unknown':
        nodes_list = graph_data.get('nodes', [])
        nodes_dict = create_nodes_dict(nodes_list)

        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù‡Ù…Ù‡ nodes Ø¯Ø± Ù‡Ù…Ø§Ù† function
        function_nodes = []
        for node in nodes_list:
            if node.get('function') == context['primary_function']:
                function_nodes.append(node['id'])

        # nodes Ú©Ù‡ Ø¯Ø± function Ù‡Ø³ØªÙ†Ø¯ ÙˆÙ„ÛŒ Ø¯Ø± path Ù†ÛŒØ³ØªÙ†Ø¯
        path_nodes_set = set(path_info['path_nodes'])
        outside_nodes = set(function_nodes) - path_nodes_set

        # Ø¢ÛŒØ§ Ø¯Ø± nodes Ø®Ø§Ø±Ø¬ Ø§Ø² pathØŒ require ÛŒØ§ protection ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŸ
        for node_id in outside_nodes:
            if node_id in nodes_dict:
                node = nodes_dict[node_id]
                node_type = node.get('type', '')
                node_label = node.get('label', '').lower()

                if 'require' in node_type.lower() or 'require' in node_label:
                    # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù‡ Ø¢ÛŒØ§ Ù‚Ø¨Ù„ Ø§Ø² source Ø§Ø³Øª (Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² line number)
                    if path_info['path_nodes']:
                        source_node_id = path_info['path_nodes'][0]
                        if source_node_id in nodes_dict:
                            source_line = nodes_dict[source_node_id].get('line_start', 999999)
                            require_line = node.get('line_start', 0)
                            if require_line < source_line:
                                context['protection_outside_path'] = True
                                break

    if debug:
        print(f"      ğŸ“‹ Context: Function={context['primary_function']}")
        print(f"         HasModifier={context['function_has_modifier']}, "
              f"Visibility={context['function_visibility']}")
        print(f"         ProtectionOutside={context['protection_outside_path']}")

    return context


def calculate_aggregate_features(path_info, enriched_info, context_info, debug=False):
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ features Ù†Ù‡Ø§ÛŒÛŒ aggregate Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ embedding
    """
    path_length = path_info['path_length']

    features = {
        # Path structure features
        'path_length_normalized': min(path_length / 20.0, 1.0),  # Normalize Ùˆ cap Ø¯Ø± 1
        'require_density': enriched_info['require_nodes_in_path'] / max(path_length, 1),
        'condition_density': enriched_info['condition_nodes_in_path'] / max(path_length, 1),
        'keccak_density': enriched_info['keccak_nodes_in_path'] / max(path_length, 1),

        # Protection features
        'mitigation_score': min(path_info['mitigation_count'] / 3.0, 1.0),  # Cap at 3
        'has_any_mitigation': int(path_info['mitigation_count'] > 0),
        'has_strong_mitigation': int(path_info['mitigation_count'] > 1),

        # Context features
        'has_modifier_protection': int(context_info['function_has_modifier']),
        'has_restricted_visibility': int(context_info['function_visibility'] in ['private', 'internal']),
        'has_external_protection': int(context_info['protection_outside_path']),
        'function_require_density': context_info['function_require_count'] / max(10, 1),  # Normalize to 10

        # Source/Sink features (encoded as categories)
        'source_type_encoded': encode_source_type(path_info['source_type']),
        'sink_type_encoded': encode_sink_type(path_info['sink_type']),

        # Complexity features
        'unique_functions_ratio': min(enriched_info['unique_functions_count'] / max(3, 1), 1.0),
        'node_diversity': len(enriched_info['node_types_count']) / max(path_length, 1),

        # Distance metrics
        'distance_to_sink': calculate_distance_to_sink(enriched_info),
        'distance_from_source': calculate_distance_from_source(enriched_info),

        # Data flow features
        'has_data_flow': int(path_info.get('has_data_flow', False)),
        'contains_loop': int(path_info.get('contains_loop', False))
    }

    if debug:
        print(f"      ğŸ“Š Aggregated Features:")
        print(f"         RequireDensity={features['require_density']:.2f}, "
              f"MitigationScore={features['mitigation_score']:.2f}")
        print(f"         ExternalProtection={features['has_external_protection']}, "
              f"ModifierProtection={features['has_modifier_protection']}")

    return features


def encode_source_type(source_type):
    """One-hot encoding Ø¨Ø±Ø§ÛŒ source types"""
    types = {
        'timestamp': [1, 0, 0, 0],
        'blockhash': [0, 1, 0, 0],
        'blocknumber': [0, 0, 1, 0],
        'block.number': [0, 0, 1, 0],
        'other': [0, 0, 0, 1]
    }
    return types.get(source_type.lower(), types['other'])


def encode_sink_type(sink_type):
    """One-hot encoding Ø¨Ø±Ø§ÛŒ sink types"""
    types = {
        'transfer': [1, 0, 0, 0],
        'randomgeneration': [0, 1, 0, 0],
        'statemodification': [0, 0, 1, 0],
        'accesscontrol': [0, 0, 0, 1],  # Ù†ÙˆØ¹ Ø¬Ø¯ÛŒØ¯ Ú©Ù‡ Ø¯Ø± Ø¯Ø§Ø¯Ù‡ Ø¯ÛŒØ¯ÛŒÙ…
        'other': [0, 0, 0, 1]
    }
    sink_lower = sink_type.lower()
    for key in types:
        if key in sink_lower or sink_lower in key:
            return types[key]
    return types['other']


def calculate_distance_to_sink(enriched_info):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ§ØµÙ„Ù‡ Ù†Ø²Ø¯ÛŒÚ©ØªØ±ÛŒÙ† require/condition ØªØ§ sink"""
    if not enriched_info['nodes_detail']:
        return 1.0

    sink_index = len(enriched_info['nodes_detail']) - 1
    min_distance = sink_index

    for i, node in enumerate(enriched_info['nodes_detail']):
        node_type = node.get('type', '').lower()
        if 'require' in node_type or node_type == 'condition':
            distance = sink_index - i
            min_distance = min(min_distance, distance)

    # Normalize
    return min_distance / max(len(enriched_info['nodes_detail']), 1)


def calculate_distance_from_source(enriched_info):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ§ØµÙ„Ù‡ Ø§ÙˆÙ„ÛŒÙ† require/condition Ø§Ø² source"""
    if not enriched_info['nodes_detail']:
        return 1.0

    for i, node in enumerate(enriched_info['nodes_detail']):
        node_type = node.get('type', '').lower()
        if 'require' in node_type or node_type == 'condition':
            # Normalize
            return i / max(len(enriched_info['nodes_detail']), 1)

    return 1.0  # Ø§Ú¯Ø± Ù‡ÛŒÚ† protection Ù†Ø¨ÙˆØ¯


def process_single_contract_paths(contract_address, base_dir):
    """
    Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø§Ù…Ù„ paths ÛŒÚ© contract Ùˆ Ø³Ø§Ø®Øª path database
    """
    print(f"\n{'=' * 70}")
    print(f"ğŸ”„ Processing Contract: {contract_address[:10]}...")
    print(f"{'=' * 70}")

    # Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ ÙØ§ÛŒÙ„
    ast_path = os.path.join(base_dir, 'contract_ast1_clean', f'{contract_address}_ast.json')
    graph_path = os.path.join(base_dir, 'contract_ast1_clean', f'{contract_address}_semantic_graph.json')
    taint_path = os.path.join(base_dir, 'contract_ast1_clean', f'{contract_address}_taint_analysis_filtered.json')
    profile_path = os.path.join(base_dir, 'contract_profiles1', f'{contract_address}_profile.json')


    result = {
        'contract_address': contract_address,
        'timestamp': datetime.now().isoformat(),
        'paths': [],
        'statistics': {
            'total_paths': 0,
            'paths_with_mitigation': 0,
            'paths_with_external_protection': 0,
            'paths_with_modifier': 0,
            'avg_path_length': 0,
            'risk_distribution': {'HIGH': 0, 'MEDIUM': 0, 'LOW': 0, 'UNKNOWN': 0}
        },
        'debug_info': {
            'status': 'pending',
            'errors': []
        }
    }

    try:
        # Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        print(f"ğŸ“– Loading data files...")
        with open(ast_path, 'r', encoding='utf-8') as f:
            ast_data = json.load(f)
        with open(graph_path, 'r', encoding='utf-8') as f:
            graph_data = json.load(f)
        with open(taint_path, 'r', encoding='utf-8') as f:
            taint_data = json.load(f)
        with open(profile_path, 'r', encoding='utf-8') as f:
            profile_data = json.load(f)

        print(f"âœ… Files loaded successfully")

        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ø± path
        paths = taint_data.get('paths', [])
        print(f"\nğŸ“Š Found {len(paths)} paths to process")

        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø­Ø¯Ø§Ú©Ø«Ø± 20 path Ø§ÙˆÙ„ Ø¨Ø±Ø§ÛŒ ØªØ³Øª (Ø¨Ø¹Ø¯Ø§Ù‹ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù‡Ù…Ù‡ Ø±Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø±Ø¯)
        max_paths = len(paths)  # Ù‡Ù…Ù‡ paths

        for idx, path_data in enumerate(paths[:max_paths]):
            print(f"\n  Path {idx + 1}/{max_paths}:")

            # 1. Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡
            path_info = extract_path_basic_info(path_data, debug=True)

            # 2. Enrichment Ø§Ø² graph
            enriched_info = enrich_from_semantic_graph(path_info, graph_data, debug=True)

            # 3. Context Ø§Ø² function
            context_info = extract_function_context(path_info, enriched_info, ast_data, graph_data, debug=True)

            # 4. Aggregate features
            aggregate_features = calculate_aggregate_features(path_info, enriched_info, context_info, debug=True)

            # ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
            path_entry = {
                'path_index': idx,
                'basic_info': path_info,
                'graph_enrichment': {
                    'nodes_detail': enriched_info['nodes_detail'],
                    'node_types_count': dict(enriched_info['node_types_count']),
                    'functions_involved': enriched_info['functions_involved'],
                    'unique_functions_count': enriched_info['unique_functions_count'],
                    'require_nodes': enriched_info['require_nodes_in_path'],
                    'condition_nodes': enriched_info['condition_nodes_in_path']
                },
                'function_context': context_info,
                'aggregate_features': aggregate_features
            }

            result['paths'].append(path_entry)

            # Ø¢Ù…Ø§Ø±Ú¯ÛŒØ±ÛŒ
            result['statistics']['total_paths'] += 1
            if path_info['mitigation_count'] > 0:
                result['statistics']['paths_with_mitigation'] += 1
            if context_info['protection_outside_path']:
                result['statistics']['paths_with_external_protection'] += 1
            if context_info['function_has_modifier']:
                result['statistics']['paths_with_modifier'] += 1

            risk = path_info['risk_level']
            if risk in result['statistics']['risk_distribution']:
                result['statistics']['risk_distribution'][risk] += 1
            else:
                result['statistics']['risk_distribution']['UNKNOWN'] += 1

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ
        if result['paths']:
            avg_length = sum(p['basic_info']['path_length'] for p in result['paths']) / len(result['paths'])
            result['statistics']['avg_path_length'] = avg_length

        result['debug_info']['status'] = 'success'

        print(f"\n{'=' * 50}")
        print(f"âœ… SUCCESS: Processed {len(result['paths'])} paths")
        print(f"ğŸ“ˆ Statistics:")
        print(f"   - Paths with mitigation: {result['statistics']['paths_with_mitigation']}")
        print(f"   - Paths with external protection: {result['statistics']['paths_with_external_protection']}")
        print(f"   - Paths with modifier: {result['statistics']['paths_with_modifier']}")
        print(f"   - Average path length: {result['statistics']['avg_path_length']:.2f}")
        print(f"   - Risk distribution: {result['statistics']['risk_distribution']}")

    except Exception as e:
        result['debug_info']['status'] = 'failed'
        result['debug_info']['errors'].append(str(e))
        print(f"\nâŒ ERROR: {str(e)}")
        import traceback
        traceback.print_exc()

    return result


def batch_process_all_contracts():
    """
    Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ù…Ù‡ contracts Ùˆ Ø³Ø§Ø®Øª path databases
    """
    base_dir = r'C:\Users\Hadis\Documents\NewModel1'
    safe_dir = os.path.join(base_dir, 'Safe_contract_clean')
    vuln_dir = os.path.join(base_dir, 'Vulnerable_contract_clean')
    output_dir = os.path.join(base_dir, 'path_databases1')
    os.makedirs(output_dir, exist_ok=True)

    # Ù„ÛŒØ³Øª contracts
    # safe_contracts = [f.replace('.sol', '') for f in os.listdir(safe_dir) if f.endswith('.sol')][:200]
    # vuln_contracts = [f.replace('.sol', '') for f in os.listdir(vuln_dir) if f.endswith('.sol')][:200]

    safe_contracts = [f.replace('.sol', '') for f in os.listdir(safe_dir) if f.endswith('.sol')][:4085]
    vuln_contracts = [f.replace('.sol', '') for f in os.listdir(vuln_dir) if f.endswith('.sol')][:223]

    print("=" * 80)
    print("ğŸš€ PATH DATABASE CONSTRUCTION - PHASE 2 (FIXED)")
    print(f"ğŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    print(f"ğŸ“Š Processing: {len(safe_contracts)} Safe + {len(vuln_contracts)} Vulnerable")
    print("-" * 80)

    global_stats = {
        'successful': 0,
        'failed': 0,
        'total_paths_processed': 0,
        'contracts_with_external_protection': 0,
        'contracts_with_modifier_protection': 0
    }

    all_contracts = [(addr, 'safe') for addr in safe_contracts] + [(addr, 'vulnerable') for addr in vuln_contracts]

    for i, (contract_address, label) in enumerate(all_contracts, 1):
        print(f"\n[{i:02d}/40] Label: {label}")
        result = process_single_contract_paths(contract_address, base_dir)

        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªÛŒØ¬Ù‡
        output_file = os.path.join(output_dir, f'{contract_address}_path_database.json')
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2)

        # Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ
        if result['debug_info']['status'] == 'success':
            global_stats['successful'] += 1
            global_stats['total_paths_processed'] += result['statistics']['total_paths']
            if result['statistics']['paths_with_external_protection'] > 0:
                global_stats['contracts_with_external_protection'] += 1
            if result['statistics']['paths_with_modifier'] > 0:
                global_stats['contracts_with_modifier_protection'] += 1
        else:
            global_stats['failed'] += 1

    # Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
    print("\n" + "=" * 80)
    print("ğŸ“Š FINAL REPORT - PATH DATABASE CONSTRUCTION")
    print("=" * 80)
    print(f"âœ… Successful: {global_stats['successful']}/40")
    print(f"âŒ Failed: {global_stats['failed']}/40")
    print(f"ğŸ“ˆ Total paths processed: {global_stats['total_paths_processed']}")
    print(f"ğŸ›¡ï¸ Contracts with external protection: {global_stats['contracts_with_external_protection']}")
    print(f"ğŸ” Contracts with modifier protection: {global_stats['contracts_with_modifier_protection']}")
    print(f"\nğŸ’¾ Path databases saved in: {output_dir}")

    # Ø°Ø®ÛŒØ±Ù‡ Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ
    stats_file = os.path.join(output_dir, 'phase2_statistics.json')
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(global_stats, f, indent=2)

    return global_stats


if __name__ == "__main__":
    # Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø±ÙˆÛŒ ÛŒÚ© contract:
    # test_address = "0x00f90986cdd79744409f8a3c7747064afa4473b5"
    # result = process_single_contract_paths(test_address, r'C:\Users\Hadis\Documents\NewModel1')

    # Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ù…Ù‡ contracts:
    batch_process_all_contracts()
